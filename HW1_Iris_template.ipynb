{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-514ec1971936>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_dummies\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import get_dummies\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Load data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_name = \"iris.data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#load the dataset into memory\n",
    "dataset = pd.read_csv(file_name,header=None,\\\n",
    "                      names=['sepal_length','sepal_width','petal_length','petal_width','species'])\n",
    "#check several examples of data\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Data preprocessing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#simple function to process the data.\n",
    "\"\"\"\n",
    "input:\n",
    "    data: iris dataset\n",
    "    num_features: 2 => select only petal width and petal width.\n",
    "                  4 => select all features\n",
    "    flower: 'Iris-setosa' => labels of Iris-setosa will be 1 and others will be 0\n",
    "            'Iris-virginica' => labels of Iris-virginica will be 1 and others will be 0\n",
    "            'None' => use one-hot encoding to represent the labels\n",
    "\n",
    "return:\n",
    "    x: normalized & shuffled data\n",
    "    y: labels\n",
    "\"\"\"\n",
    "def data_preprocessing(data,num_features,flower=None):\n",
    "    if num_features==2:\n",
    "        features = data.columns[2:4]\n",
    "    else:\n",
    "        features = data.columns[0:4]\n",
    "    labels = dataset.columns[4]\n",
    "    print(features)\n",
    "    print(labels)\n",
    "    #normalize the data\n",
    "    data_norm = pd.DataFrame(data)\n",
    "    for feature in features:\n",
    "        data[feature] = (data[feature]-data[feature].mean())/data[feature].std()\n",
    "\n",
    "    #shuffle the data\n",
    "    indices = data_norm.index.tolist()\n",
    "    indices = np.array(indices)\n",
    "    np.random.shuffle(indices)\n",
    "    x = data_norm.reindex(indices)[features]\n",
    "    y = data_norm.reindex(indices)[labels]\n",
    "    if flower=='Iris-setosa':\n",
    "        for i in range(len(y)):\n",
    "            y[i]=1 if y[i]=='Iris-setosa' else 0\n",
    "        y = y.values.reshape(len(y),1)\n",
    "    elif flower=='Iris-virginica':\n",
    "        for i in range(len(y)):\n",
    "            y[i]=1 if y[i]=='Iris-virginica' else 0\n",
    "        y = y.values.reshape(len(y),1)\n",
    "    else:\n",
    "        y = get_dummies(y)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Part a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9ad3ec572d32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Iris-setosa'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#generate traning and validation sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx_train_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "x_1,y_1 = data_preprocessing(dataset,2,flower='Iris-setosa')\n",
    "#generate traning and validation sets\n",
    "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(x_1,y_1,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Define hyper-parameters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Define Placeholder and Variables</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-32c10e2541c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#tf graph input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#define the weights and initialized with random normal distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,2],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,1],name='Y')\n",
    "#define the weights and initialized with random normal distribution\n",
    "W = tf.Variable(tf.random_normal([2,1]),tf.float32,name='W')\n",
    "#define the bias with zero initialization\n",
    "b = tf.Variable(tf.zeros([1,1]),tf.float32,name='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Define update rule and accuracy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5f9db31bc0ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Iris-setosa'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data_preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "#define the predicted output label, Tensorflow doesn't have step function, we use tf.round to do that\n",
    "Y_hat = tf.round(tf.sigmoid(tf.add(tf.matmul(X,W),b)))\n",
    "#the error between target and logit\n",
    "error = Y-Y_hat\n",
    "\n",
    "#weight and bias update rule w(t+1) = w(t) + learning_rate * error * x\n",
    "dW = tf.matmul(tf.transpose(X),error)\n",
    "db = tf.reduce_sum(error,0)\n",
    "W_ = W + lr*dW\n",
    "b_ = b + lr*db\n",
    "\n",
    "#group two operations together\n",
    "step = tf.group(W.assign(W_), b.assign(b_))\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(Y_hat,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "C = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Execute training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#initialize tensorflow variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#start tensorflow session\n",
    "with tf.Session() as sess:\n",
    "    #run initialization\n",
    "    sess.run(init)\n",
    "    #training for number of iterations\n",
    "    for i in range(epochs):\n",
    "        #every iteration we run optimization on the training data X and labels Y\n",
    "        sess.run(step,feed_dict = {X:x_train_1,Y:y_train_1})\n",
    "        c = sess.run(error,feed_dict = {X:x_train_1,Y:y_train_1})\n",
    "        C.append(c)\n",
    "        if i % 10 ==0:\n",
    "            acc = sess.run(accuracy,feed_dict={X:x_train_1, Y:y_train_1})\n",
    "            print(\"step \"+str(i)+\", Accuracy= {:.3f}\".format(acc))\n",
    "    #after training, we acquire the weight and bias\n",
    "    #np.squeeze plays a role to get rid of the redundant dimension [i.e.make (2,1) be (2,)]\n",
    "    W = np.squeeze(sess.run(W))\n",
    "    b = np.squeeze(sess.run(b))\n",
    "    pred=sess.run(Y_hat,feed_dict = {X:x_test_1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Plot</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train_1 = x_train_1.values\n",
    "y_test_1 = y_test_1.astype('float32')\n",
    "print(type(y_test_1[0][0]),type(pred[0][0]))\n",
    "cm=confusion_matrix(y_test_1,pred)\n",
    "print(cm)\n",
    "succ = cm[0][0]+cm[1][1]\n",
    "fail = cm[0][1]+cm[1][0]\n",
    "if fail == 0:\n",
    "    acc = 1\n",
    "else:\n",
    "    acc = succ/(succ+fail)\n",
    "print('test set accuracy:',acc)\n",
    "plot_x = np.array([np.min(x_train_1[:, 0] - 0.2), np.max(x_train_1[:, 1]+0.2)])\n",
    "plot_y = 1 / W[1] * (-W[0] * plot_x - b)\n",
    "\n",
    "plt.scatter(x_train_1[:, 0], x_train_1[:, 1], c=np.squeeze(y_train_1), s=100, cmap='viridis')\n",
    "plt.plot(plot_x, plot_y, color='k', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification graph shows that the two classes (setosa and non-setosa) are relatively far away. This means that in these three species of flowers, two of them have very close features. The accuracy on test data is 100%, which is reasonable because of the very different features of setosa and non-setosa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Part b:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_2,y_2 = data_preprocessing(dataset,2,flower='Iris-virginica')\n",
    "#generate traning and validation sets\n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(x_2,y_2,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Define hyper-parameter</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Define Placeholder and Variables</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#tf graph input\n",
    "X2 = tf.placeholder(tf.float32,[None,2],name='X2')\n",
    "Y2 = tf.placeholder(tf.float32,[None,1],name='Y2')\n",
    "#define the weights and initialized with random normal distribution\n",
    "W2 = tf.Variable(tf.random_normal([2,1]),tf.float32,name='W2')\n",
    "#define the bias with zero initialization\n",
    "b2 = tf.Variable(tf.zeros([1,1]),tf.float32,name='b2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Define update rule and accuracy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#define the predicted output label, Tensorflow doesn't have step function, we use tf.round to do that\n",
    "Y_hat2 = tf.round(tf.sigmoid(tf.add(tf.matmul(X2,W2),b2)))\n",
    "#the error between target and logit\n",
    "error2 = Y2-Y_hat2\n",
    "\n",
    "#weight and bias update rule w(t+1) = w(t) + learning_rate * error * x\n",
    "dW2 = tf.matmul(tf.transpose(X2),error2)\n",
    "db2 = tf.reduce_sum(error2,0)\n",
    "W_2 = W2 + lr*dW2\n",
    "b_2 = b2 + lr*db2\n",
    "\n",
    "#group two operations together\n",
    "step2 = tf.group(W2.assign(W_2), b2.assign(b_2))\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred2 = tf.equal(tf.argmax(Y_hat2,1),tf.argmax(Y2,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy2 = tf.reduce_mean(tf.cast(correct_pred2,tf.float32),name='accuracy2')\n",
    "C2 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Execute training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#initialize tensorflow variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#start tensorflow session\n",
    "with tf.Session() as sess:\n",
    "    #run initialization\n",
    "    sess.run(init)\n",
    "    #training for number of iterations\n",
    "    for i in range(epochs):\n",
    "        #every iteration we run optimization on the training data X and labels Y\n",
    "        sess.run(step2,feed_dict = {X2:x_train_2,Y2:y_train_2})\n",
    "        c2 = sess.run(error2,feed_dict = {X2:x_train_2,Y2:y_train_2})\n",
    "        C2.append(c2)\n",
    "        if i % 10 ==0:\n",
    "            acc = sess.run(accuracy2,feed_dict={X2:x_train_2, Y2:y_train_2})\n",
    "            print(\"step \"+str(i)+\", Accuracy= {:.3f}\".format(acc))\n",
    "    #after training, we acquire the weight and bias\n",
    "    #np.squeeze plays a role to get rid of the redundant dimension [i.e.make (2,1) be (2,)]\n",
    "    W2 = np.squeeze(sess.run(W2))\n",
    "    b2 = np.squeeze(sess.run(b2))\n",
    "    pred2=sess.run(Y_hat2,feed_dict = {X2:x_test_2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Plot</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8034cbf9b6c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_train_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_test_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_test_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train_2' is not defined"
     ]
    }
   ],
   "source": [
    "x_train_2 = x_train_2.values\n",
    "y_test_2 = y_test_2.astype('float32')\n",
    "print(type(y_test_2[0][0]),type(pred2[0][0]))\n",
    "cm=confusion_matrix(y_test_2,pred2)\n",
    "print(cm)\n",
    "succ = cm[0][0]+cm[1][1]\n",
    "fail = cm[0][1]+cm[1][0]\n",
    "if fail == 0:\n",
    "    acc = 1\n",
    "else:\n",
    "    acc = succ/(succ+fail)\n",
    "print('test set accuracy:',acc)\n",
    "plot_x = np.array([np.min(x_train_2[:, 0] - 0.2), np.max(x_train_2[:, 1]+0.2)])\n",
    "plot_y = 1 / W2[1] * (-W2[0] * plot_x - b2)\n",
    "\n",
    "plt.scatter(x_train_2[:, 0], x_train_2[:, 1], c=np.squeeze(y_train_2), s=100, cmap='viridis')\n",
    "plt.plot(plot_x, plot_y, color='k', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on test data (93%) is lower than part a because the species of Virginica and Versicolor have similar features which causes error at the boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Part c:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_3,y_3 = data_preprocessing(dataset,4)\n",
    "#generate traning and validation sets\n",
    "x_train_3, x_test_3, y_train_3, y_test_3 = train_test_split(x_3,y_3,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Define hyper-parameters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Define placehoder and variables</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#tf graph input\n",
    "X3 = tf.placeholder(tf.float32,[None,4],name='X3')\n",
    "Y3 = tf.placeholder(tf.float32,[None,3],name='Y3')\n",
    "#define the weights and initialized with random normal distribution\n",
    "W1 = tf.Variable(tf.random_normal([4,256]),tf.float32,name='W1')\n",
    "W2 = tf.Variable(tf.random_normal([256,128]),tf.float32,name='W2')\n",
    "W3 = tf.Variable(tf.random_normal([128,3]),tf.float32,name='W3')\n",
    "#define the bias with zero initialization\n",
    "b1 = tf.Variable(tf.zeros([256]),tf.float32,name='b1')\n",
    "b2 = tf.Variable(tf.zeros([128]),tf.float32,name='b2')\n",
    "b3 = tf.Variable(tf.zeros([3]),tf.float32,name='b3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Define the neural network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def forward(x,w1,b1,w2,b2,w3,b3,train=True):\n",
    "    Z = tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "    Z1 = tf.nn.relu(tf.matmul(Z,w2)+b2)\n",
    "    Z2 = tf.matmul(Z1,w3)+b3\n",
    "    \n",
    "    if train:\n",
    "        return Z2\n",
    "    else:\n",
    "        return tf.nn.softmax(Z2)\n",
    "y_hat = forward(X3,W1,b1,W2,b2,W3,b3)\n",
    "predc = forward(X3,W1,b1,W2,b2,W3,b3,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Define cost function and accuracy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=y_hat,labels=y_train_3))\n",
    "train = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred3 = tf.equal(tf.argmax(y_hat,1),tf.argmax(Y3,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy3 = tf.reduce_mean(tf.cast(correct_pred3,tf.float32),name='accuracy3')\n",
    "C3 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Execute training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#initialize tensorflow variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#start tensorflow session\n",
    "with tf.Session() as sess:\n",
    "    #run initialization\n",
    "    sess.run(init)\n",
    "    #training for number of iterations\n",
    "    for i in range(epochs):\n",
    "        #every iteration we run optimization on the training data X and labels Y\n",
    "        sess.run(train,feed_dict = {X3:x_train_3,Y3:y_train_3})\n",
    "        c3 = sess.run(cost,feed_dict = {X3:x_train_3,Y3:y_train_3})\n",
    "        C3.append(c3)\n",
    "        if i % 10 ==0:\n",
    "            acc = sess.run(accuracy3,feed_dict={X3:x_train_3, Y3:y_train_3})\n",
    "            print(\"step \"+str(i)+\", Accuracy= {:.3f}\".format(acc))\n",
    "    pred3=sess.run(predc,feed_dict = {X3:x_test_3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Plot</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#accuracy on test data\n",
    "pred3 = np.round(pred3)\n",
    "x_train_3 = x_train_3.values\n",
    "y_test_3 = y_test_3.values\n",
    "y_test_3 = y_test_3.astype('float32')\n",
    "count = 0\n",
    "total = 0\n",
    "for i in range(len(pred3)):\n",
    "    if pred3[i].all() == y_test_3[i].all():\n",
    "        count+=1\n",
    "    total+=1\n",
    "    \n",
    "acc = count/total\n",
    "print('Testing ACcuracy:',acc)\n",
    "#plot cost\n",
    "plt.plot(C3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
